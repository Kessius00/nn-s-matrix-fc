{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_QIHoPUjwKgZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def frobenius_norm(A):\n",
    "    return np.linalg.norm(A, 'fro')\n",
    "\n",
    "def svd_threshold(A, threshold):\n",
    "    \"\"\"\n",
    "    Apply the thresholding operator to the singular values of A\n",
    "    A: matrix to apply the thresholding operator\n",
    "    threshold: threshold value to apply to the singular values\n",
    "    Returns reconstructed matrix A with singular values thresholded\n",
    "    \"\"\"\n",
    "    U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "    s = np.maximum(s - threshold, 0)\n",
    "    return np.dot(U, np.dot(np.diag(s), V))\n",
    "\n",
    "def sample_from_matrix(Matrix_to_sample, Ratio_to_keep, seed=42):\n",
    "    '''\n",
    "    Samples frpm a matrix A, keeping a ratio_to_keep of the values\n",
    "    A: matrix to sample from\n",
    "    ratio_to_keep: ratio of values to keep in the sampled matrix\n",
    "    seed: seed for the random number generator, for ease of reproducibility\n",
    "    Returns the sampled matrix X, the coordinates of the sampled values, and a boolean matrix of the same size as A, with True values where we have sampled the matrix A\n",
    "    '''\n",
    "    # initialize the random number generator with the seed\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # calculate the number of samples to keep\n",
    "    num_samples = int(Ratio_to_keep * Matrix_to_sample.size)\n",
    "\n",
    "    sampled_indices = rng.choice(np.arange(Matrix_to_sample.size), size=num_samples, replace=False) # sample the indices of the values to keep\n",
    "    sampled_mask = np.zeros(Matrix_to_sample.shape, dtype=bool) # create a boolean mask of the same size as the matrix\n",
    "    sampled_mask.flat[sampled_indices] = True # set the values to True where we have sampled the matrix (true if we have sampled the value)\n",
    "\n",
    "    sampled_matrix = np.zeros(Matrix_to_sample.shape) # create a matrix of zeros of the same size as the matrix to sample\n",
    "    sampled_matrix.flat[sampled_indices] = Matrix_to_sample.flat[sampled_indices] # set the values of the sampled matrix to the values of the original matrix\n",
    "\n",
    "\n",
    "    return sampled_matrix, sampled_indices, sampled_mask\n",
    "\n",
    "def normalize_rows(X):\n",
    "    '''\n",
    "    Normalize the rows of a matrix X, by dividing each row by the maximum value of the row (max normalization)\n",
    "    '''\n",
    "    row_max = X.max(axis=1)\n",
    "    X = X / row_max[:, np.newaxis]\n",
    "    return X\n",
    "\n",
    "def normalize_columns(X):\n",
    "    '''\n",
    "    Normalize the columns of a matrix X, by dividing each column by the maximum value of the column (max normalization)\n",
    "    '''\n",
    "    col_max = X.max(axis=0)\n",
    "    X = X / col_max[np.newaxis, :]\n",
    "    return X\n",
    "\n",
    "def insampling_error_relative(X_sampled, X_reconstructed, Sampled_mask, norm='fro'):\n",
    "    \"\"\"\n",
    "    Compute the relative insampling error between the original matrix X_sampled and the reconstructed matrix X_reconstructed\n",
    "    X_sampled: the sampled matrix with zeros where we have not sampled\n",
    "    X_reconstructed: reconstructed/estimated matrix we want to compare to X_sampled\n",
    "    Sampled_mask: boolean matrix of the same size as X_sampled, with True values where we have sampled the matrix X_sampled. If you use the function sample_from_matrix, you can use the third output of the function\n",
    "    \"\"\"\n",
    "\n",
    "    X_reconstructed_sampled = np.where(Sampled_mask, X_reconstructed, 0) # sample the reconstructed matrix\n",
    "    error = np.linalg.norm(X_sampled - X_reconstructed_sampled, ord=norm)\n",
    "    relative_error = error / np.linalg.norm(X_sampled, ord=norm)\n",
    "\n",
    "    return relative_error\n",
    "\n",
    "def out_of_sample_error_relative(X_original, X_reconstructed, Sampled_mask, norm='fro'):\n",
    "    \"\"\"\n",
    "    Compute the relative out-of-sample error between the original matrix X_original and the reconstructed matrix X_reconstructed divided by the complement of the sampled matrix\n",
    "    X_original: the original matrix\n",
    "    X_reconstructed: reconstructed/estimated matrix we want to compare to X_original\n",
    "    Sampled_mask: boolean matrix of the same size as X_original, with True values where we have sampled the matrix X_original. If you use the function sample_from_matrix, you can use the third output of the function\n",
    "    \"\"\"\n",
    "    X_original_complement = np.where(~Sampled_mask, X_original, 0)\n",
    "    X_reconstructed_complement = np.where(~Sampled_mask, X_reconstructed, 0)\n",
    "    error = np.linalg.norm(X_original_complement - X_reconstructed_complement, ord=norm)\n",
    "    relative_error = error / np.linalg.norm(X_original_complement, ord=norm)\n",
    "    return relative_error\n",
    "\n",
    "def general_error_relative(X_original, X_reconstructed, norm='fro'):\n",
    "    \"\"\"\n",
    "    Compute the relative error between the original matrix X_original and the reconstructed matrix X_reconstructed\n",
    "    X_original: the original matrix\n",
    "    X_reconstructed: reconstructed/estimated matrix we want to compare to X_original\n",
    "    \"\"\"\n",
    "    error = np.linalg.norm(X_original - X_reconstructed, ord=norm)\n",
    "    relative_error = error / np.linalg.norm(X_original, ord=norm)\n",
    "\n",
    "    return relative_error\n",
    "\n",
    "def euclideanDivergence(x,y):\n",
    "    return .5*(x-y)**2\n",
    "\n",
    "def KLDivergence(x,y):\n",
    "    # the Kullback-Leibler divergence function\n",
    "    return x * np.log(x/y)-x+y\n",
    "\n",
    "def ISDivergence(x,y):\n",
    "    # the Itakura-Saito divergence function\n",
    "    return x/y - np.log(x/y)-1\n",
    "\n",
    "def positive_projection(F):\n",
    "    # for loc in np.argwhere(F<0):\n",
    "    #     F[loc[0],loc[1]] = 0\n",
    "    return np.maximum(F, 0)\n",
    "\n",
    "def calculate_sparsity(X):\n",
    "    '''\n",
    "    Calculate the sparsity of a matrix X, defined as the percentage of zero elements in the matrix\n",
    "    X: the matrix to calculate the sparsity of\n",
    "    '''\n",
    "    total_elements = X.size\n",
    "    zero_elements = np.count_nonzero(X == 0)\n",
    "    sparsity = (zero_elements / total_elements) * 100\n",
    "    return sparsity\n",
    "\n",
    "def compute_gradient_H(Wt, Y, X, E, beta):\n",
    "    return np.dot(np.dot(Wt.T, Wt), Y) - np.dot(Wt.T, X) + np.dot(beta, E)\n",
    "\n",
    "def compute_gradient_W(Ht, Y, X):\n",
    "    return np.dot(np.dot(Y, Ht), Ht.T) - np.dot(X, Ht.T)\n",
    "\n",
    "def OGM_calculate_objective_function(W, H, X):\n",
    "    re_er = np.linalg.norm(X - np.dot(W, H), 'fro')\n",
    "    data_norm_squared = np.linalg.norm(X, 'fro')\n",
    "    objective_percentage = (re_er / data_norm_squared) * 100\n",
    "    return objective_percentage\n",
    "\n",
    "def OGM_H(Wt, Ht, X, E, beta, max_iter=100, epsilon_h=0.01):\n",
    "    Y = deepcopy(Ht)\n",
    "    alpha = 1\n",
    "    L = np.linalg.norm(np.dot(Wt.T, Wt), 2)\n",
    "    k = 0\n",
    "    H_prev = deepcopy(Ht)\n",
    "    grad_norm_list_H = []\n",
    "\n",
    "    while k < max_iter:\n",
    "        grad_F = compute_gradient_H(Wt, Y, X, E, beta)\n",
    "        H = positive_projection(Y - (1/L) * grad_F)\n",
    "        grad_norm = frobenius_norm(positive_projection(grad_F))\n",
    "        grad_norm_list_H.append(grad_norm)\n",
    "\n",
    "        if grad_norm <= epsilon_h:\n",
    "            break\n",
    "\n",
    "        alpha_next = (1 + np.sqrt(4 * alpha**2 + 1)) / 2\n",
    "\n",
    "        if k > 0:\n",
    "            Y = H + ((alpha - 1) / alpha_next) * (H - H_prev)\n",
    "        else:\n",
    "            Y = H\n",
    "\n",
    "        alpha = alpha_next\n",
    "        H_prev = deepcopy(H)\n",
    "        k += 1\n",
    "\n",
    "    return H, grad_norm_list_H\n",
    "\n",
    "def OGM_W(Wt, Ht, X, max_iter=100, epsilon_w=0.01):\n",
    "    Y = deepcopy(Wt)\n",
    "    alpha = 1\n",
    "    L = np.linalg.norm(np.dot(Ht, Ht.T), 2)\n",
    "    k = 0\n",
    "    W_prev = deepcopy(Wt)\n",
    "    grad_norm_list_W = []\n",
    "\n",
    "    while k < max_iter:\n",
    "        grad_F = compute_gradient_W(Ht, Y, X)\n",
    "        W = positive_projection(Y - (1/L) * grad_F)\n",
    "        grad_norm = frobenius_norm(positive_projection(grad_F))\n",
    "        grad_norm_list_W.append(grad_norm)\n",
    "\n",
    "        if grad_norm <= epsilon_w:\n",
    "            break\n",
    "\n",
    "        alpha_next = (1 + np.sqrt(4 * alpha**2 + 1)) / 2\n",
    "\n",
    "        if k > 0:\n",
    "            Y = W + ((alpha - 1) / alpha_next) * (W - W_prev)\n",
    "        else:\n",
    "            Y = W\n",
    "\n",
    "        alpha = alpha_next\n",
    "        W_prev = deepcopy(W)\n",
    "        k += 1\n",
    "\n",
    "    return W, grad_norm_list_W\n",
    "\n",
    "def OGM_calculate_objective_function_observed(W, H, X, Omega):\n",
    "    re_er = np.linalg.norm(np.multiply(Omega, (X - np.dot(W, H))), 'fro')\n",
    "    data_norm_squared = np.linalg.norm(np.multiply(Omega, X), 'fro')\n",
    "    objective_percentage = (re_er / data_norm_squared) * 100\n",
    "    return objective_percentage\n",
    "\n",
    "def OGM_calculate_objective_function_unobserved(W, H, X, Omega):\n",
    "    re_er = np.linalg.norm(np.multiply((1-Omega), (X - np.dot(W, H))), 'fro')\n",
    "    data_norm_squared = np.linalg.norm(np.multiply((1-Omega), X), 'fro')\n",
    "    objective_percentage = (re_er / data_norm_squared) * 100\n",
    "    return objective_percentage\n",
    "\n",
    "def k_0_finder(tau, step_size, sampled_entries):\n",
    "    \"\"\"\n",
    "    Find the initial value of k_0.\n",
    "    \"\"\"\n",
    "    return np.ceil(tau / (step_size * np.linalg.norm(sampled_entries, 2)))\n",
    "\n",
    "def projection_operator(M_sampled, X):\n",
    "    \"\"\"\n",
    "    Compute the projection operator for matrix completion.\n",
    "    \"\"\"\n",
    "    filled_M_sampled = np.where(M_sampled == 0, X, M_sampled)\n",
    "    projection = X - filled_M_sampled\n",
    "    return projection\n",
    "\n",
    "def projection_mask_operator(A, M):\n",
    "    # Assuming M contains the mask of observed elements\n",
    "    mask = (M != 0)\n",
    "    return A * mask\n",
    "\n",
    "def suggested_stop(X_k, original_sampled, tolerance):\n",
    "    \"\"\"\n",
    "    Check the stopping condition based on the relative difference between X_k and the original sampled matrix.\n",
    "    \"\"\"\n",
    "    rel_error = np.linalg.norm(projection_operator(original_sampled, X_k), 'fro') / np.linalg.norm(original_sampled, 'fro')\n",
    "    # print(\"Current relative in-sampling error:\", rel_error)\n",
    "    return rel_error <= tolerance\n",
    "\n",
    "def generate_simulated_IMS_matrix(m=50000, n=500, rank=100, seed=42, random_mean=1.0, random_scale=1, batch_contrast=2, batch_abondance=200, remove_batches=True):\n",
    "    \"\"\"\n",
    "    DEPRECATED: DON'T USE\n",
    "    m: number of rows\n",
    "    n: number of columns\n",
    "    rank: rank of the matrix, constructed from 2 matrices W and H of size m x rank and rank x n\n",
    "    seed: seed for the random number generator, for ease of reproducibility\n",
    "    random_mean: mean of the gamma distribution for the random values\n",
    "    random_scale: scale of the gamma distribution for the random values\n",
    "    batch_contrast: contrast of the batch values, during generation, 'batches' are added to the W and H matrices, to simulate the original data\n",
    "    batch_abondance: number of batches to add to the W and H matrices\n",
    "    remove: if True, remove some values in the batches, to simulate the original data\n",
    "\n",
    "    Run the function to generate a simulated IMS matrix, with the given parameters\n",
    "    The function makes first two matrices W and H of size m x rank and rank x n, and then computes the product W*H, which is the simulated IMS matrix\n",
    "    Run the function with no input to get the default values\n",
    "    \"\"\"\n",
    "\n",
    "    rng=np.random.default_rng(seed)\n",
    "    W_rows, W_cols = m, rank\n",
    "    H_rows, H_cols = rank, n\n",
    "\n",
    "    W = rng.gamma(shape=random_mean, scale=random_scale, size=(W_rows, W_cols))\n",
    "    H = rng.gamma(shape=random_mean, scale=random_scale/100, size=(H_rows, H_cols))\n",
    "\n",
    "    def add_batches(A,num_batches, batch_size, high_value, remove=True):\n",
    "        for _ in range(num_batches):\n",
    "            row_start = np.random.randint(0, A.shape[0] - batch_size[0])\n",
    "            col_start = np.random.randint(0, A.shape[1] - batch_size[1])\n",
    "            A[row_start:row_start + batch_size[0], col_start:col_start + batch_size[1]] += high_value+np.random.normal(0,high_value/2,batch_size)\n",
    "        # Set some values to zero in the batches\n",
    "        if remove:\n",
    "            for _ in range(num_batches):\n",
    "                row_start = np.random.randint(0, A.shape[0] - batch_size[0])\n",
    "                col_start = np.random.randint(0, A.shape[1] - batch_size[1])\n",
    "                A[row_start:row_start + batch_size[0], col_start:col_start + batch_size[1]] = 1e-2\n",
    "        A = np.clip(A, 1e-2, None)\n",
    "        return A\n",
    "\n",
    "    W = add_batches(W, num_batches=batch_abondance, batch_size=(np.max([int(m/5),1]), 2), high_value=batch_contrast,remove=remove_batches)\n",
    "    H = add_batches(H, num_batches=batch_abondance*5, batch_size=(2,np.max([int(n/25),1])), high_value=batch_contrast/40, remove=remove_batches)\n",
    "    simulated_X = np.dot(W,H)\n",
    "    simulated_X = normalize_columns(simulated_X)\n",
    "    return simulated_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmgXsoloKlce"
   },
   "source": [
    "#Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vG1G4mCCu9Kj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, norm\n",
    "\n",
    "\n",
    "def NMFC(M, k,Sampled_mask, alpha, beta, max_iter, epsilon):\n",
    "    '''\n",
    "    Factorize a matrix M into two non-negative matrices X and Y using the Alternating Direction Method of Multipliers (ADMM) algorithm. As defined in the paper: doi: 10.1007/s11464-012-0194-5\n",
    "    M: the matrix to factorize (with missing entries)\n",
    "    k: the rank of the factorization\n",
    "    alpha_factor: the factor to scale the alpha parameter\n",
    "    alpha_choice: the choice of alpha parameter calculation, 1 or 2\n",
    "    max_iter: the maximum number of iterations\n",
    "    epsilon: the tolerance for the convergence criterion\n",
    "    returns: W, H the factorized matrices, W: m x k, H: k x n\n",
    "    '''\n",
    "\n",
    "    A = projection_mask_operator(M, M)\n",
    "    m, n = A.shape\n",
    "\n",
    "    frobenius_A = norm(A, 'fro')\n",
    "\n",
    "    # if alpha_choice == 1:\n",
    "    #     alpha = alpha_factor * frobenius_A * max(m, n) / k\n",
    "    #     beta = (n * alpha) / m\n",
    "\n",
    "    # elif alpha_choice == 2:\n",
    "    #     alpha = alpha_factor\n",
    "    #     beta = alpha\n",
    "\n",
    "\n",
    "    # print(f'alpha: {alpha}   beta: {beta}')\n",
    "\n",
    "    gamma = 1.618\n",
    "\n",
    "    # Initialize matrices X, Y, Z, U, V, Lambda, and Pi\n",
    "    W = np.random.rand(m, k)\n",
    "    H = np.random.rand(k, n)\n",
    "    Z = A.copy()\n",
    "    U = np.zeros(W.shape)\n",
    "    V = np.zeros(H.shape)\n",
    "    # lagrange multipliers\n",
    "    Lambda = np.zeros(W.shape)\n",
    "    Pi = np.zeros(H.shape)\n",
    "\n",
    "\n",
    "\n",
    "#ADMM algorithm\n",
    "    intermediate_error = []\n",
    "    for i in range(max_iter):\n",
    "        W_next = positive_projection((Z @ H.T + alpha * U - Lambda) @ inv(H @ H.T + alpha * np.eye(k)))\n",
    "\n",
    "        H_next = positive_projection(inv(W_next.T @ W_next + beta * np.diag(np.ones(k))) @ (W_next.T @ Z + beta * V - Pi))\n",
    "\n",
    "        Z_next = W_next @ H_next + projection_mask_operator(M - (W_next @ H_next), M)\n",
    "\n",
    "        U_next = positive_projection(W_next + Lambda / alpha)\n",
    "        V_next = positive_projection(H_next + Pi / beta)\n",
    "\n",
    "        Lambda_next = Lambda + gamma * alpha * (W_next - U_next)\n",
    "        Pi_next = Pi + gamma * beta * (H_next - V_next)\n",
    "\n",
    "        f_k = norm(projection_mask_operator(W @ H - A, M), 'fro') / frobenius_A\n",
    "        f_k1 = norm(projection_mask_operator(W_next @ H_next - A, M), 'fro') / frobenius_A\n",
    "        intermediate_error.append(insampling_error_relative(M, W_next @ H_next, Sampled_mask))\n",
    "        print(i, insampling_error_relative(M, W_next @ H_next, Sampled_mask))\n",
    "\n",
    "        # Convergence check based on the relative change in Frobenius norm between iterations\n",
    "        if i > 200:\n",
    "          if np.abs(f_k1 - f_k) / np.maximum(1, np.abs(f_k)) <= epsilon:\n",
    "              print('first', i)\n",
    "              return W_next, H_next, intermediate_error\n",
    "\n",
    "        # Convergence check based on the relative change in Frobenius norm of the projection operator\n",
    "        elif f_k <= epsilon:\n",
    "            print('second', i)\n",
    "\n",
    "            return W_next, H_next, intermediate_error\n",
    "\n",
    "        W, H, Z, U, V, Lambda, Pi = W_next, H_next, Z_next, U_next, V_next, Lambda_next, Pi_next\n",
    "    print('convergence criteria not met')\n",
    "\n",
    "    return W, H, intermediate_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
